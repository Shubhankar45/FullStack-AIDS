{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82fd3003-1547-4a52-9678-fdcbf4420d7e",
   "metadata": {},
   "source": [
    "| Model        | Description                                              |\n",
    "| ------------ | -------------------------------------------------------- |\n",
    "| `llama3`     | Meta's LLaMA 3 (most powerful open-source LLM in Ollama) |\n",
    "| `llama2`     | Previous generation, still powerful and faster           |\n",
    "| `llama2:13b` | 13 billion parameter version                             |\n",
    "| `llama2:7b`  | Lighter, faster, smaller RAM usage                       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0421c40-cb81-411c-ab58-def1f0768eec",
   "metadata": {},
   "source": [
    "| Model          | Description                                           |\n",
    "| -------------- | ----------------------------------------------------- |\n",
    "| `mistral`      | Mistral 7B — small, fast, very good accuracy          |\n",
    "| `mixtral`      | Mixtral 8x7B (Mixture of Experts, higher performance) |\n",
    "| `mixtral:8x7b` | Explicit version name                                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb56c8b-f0ad-439c-b45e-deb1bed8b1a7",
   "metadata": {},
   "source": [
    "| Model      | Description                   |\n",
    "| ---------- | ----------------------------- |\n",
    "| `gemma:2b` | Lightweight Google open model |\n",
    "| `gemma:7b` | More capable version          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0427684-6211-4af8-bb71-51dd362536f7",
   "metadata": {},
   "source": [
    "| Model             | Description                                        |\n",
    "| ----------------- | -------------------------------------------------- |\n",
    "| `tinyllama`       | Super lightweight (1.1B) for low-resource use      |\n",
    "| `phi`             | Microsoft's Phi-2 (small but surprisingly capable) |\n",
    "| `llava`           | Vision + language (multi-modal)                    |\n",
    "| `orca-mini`       | Lightweight distilled models                       |\n",
    "| `dolphin-mistral` | Chat-tuned Mistral variant                         |\n",
    "| `openhermes`      | Open-source assistant model                        |\n",
    "| `zephyr`          | Chat-optimized, fine-tuned small model             |\n",
    "| `codellama`       | Code generation model (by Meta)                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528fb18e-d279-4f9e-9147-a07d672370ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d566f3ce-a13c-433e-9956-e66b57bdd1b6",
   "metadata": {},
   "source": [
    "**ollama chat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "933edf2e-9e14-4998-83d4-812bff6b3684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning (ML) is the process of learning from data without knowing the true purpose or function of the data. It involves the use of algorithms to learn and predict patterns in data based on previous observations, without directly providing guidance or instruction. In other words, ML allows machines or computers to perform tasks based on observed behaviors or relationships.\n",
      "\n",
      "Here's a simple explanation of how machine learning works: \n",
      "\n",
      "1. Collect and prepare the data - You have collected a lot of data that you can use to train an algorithm. These data are the \"training data.\" The purpose is to learn patterns from this data. \n",
      "\n",
      "2. Define the objective - You define what you want to achieve with your machine learning model, which could be predicting future events, detecting fraud or recommending products.\n",
      "\n",
      "3. Create an algorithm - Once you have collected and prepared the training data, you can use statistical techniques to create an algorithm based on that data.\n",
      "\n",
      "4. Test the algorithm - After the algorithm is created, you perform a test dataset (usually smaller than the entire data set) and compare its predictions with the true outcome. This feedback helps you improve your model.\n",
      "\n",
      "5. Train the algorithm - If the performance of the algorithm is good, you can use this trained model to predict future events, detect fraud or recommend products.\n",
      "\n",
      "In essence, machine learning is a way of using data to learn and make predictions, without being explicitly programmed by humans.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='tinyllama',\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'What is machine learning?'}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a52f52b9-d1c2-4423-9e5a-1bd05a7b4aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Musk is an American business magnate, entrepreneur, inventor, and philanthropist. He is best known for being the founder and CEO of SpaceX, Tesla Inc., Neuralink, The Boring Company, SolarCity, PayPal, and a number of other successful companies. Musk has received numerous awards and accolades throughout his career, including a Nobel Prize in Physics for \"his contributions to clean energy technology.\"\n",
      "\n",
      "Musk is also the founder and lead singer of rock band Tesla, which he founded with his brother. He also co-founded rocket company SpaceX, which has been successful in developing reusable launch vehicles and spacecraft. Musk is known for his bold and visionary plans for the future of humanity, which include advancements in renewable energy sources, artificial intelligence, and colonizing Mars.\n",
      "Elon Musk owns several companies as follows:\n",
      "1. SpaceX (Space Transportation System) - Aerospace company specializing in launch vehicles and space transportation services\n",
      "2. Tesla (Transportation Enterprise Services) - Electric vehicle company\n",
      "3. Neuralink (Neural Interface Technology) - Neuroengineering firm aiming to create brain-computer interfaces to help people with brain injuries\n",
      "4. SolarCity (Solar Power Generation and Storage) - Renewable energy company selling solar panels and other renewable energy products\n",
      "5. X.com (PayPal Holdings) - Online payment system and e-commerce company bought by PayPal, Inc. In 2002\n",
      "6. The Boring Company (TBC) - Infrastructure company that designs, finances, and operates tunnels under the city, including the tunnel boring machine named \"Brick.\"\n",
      "7. Terraform Labs (TFL) - Renewable energy company aiming to create hydroelectric power using geothermal sources for long-term energy storage\n",
      "8. DraftKings (DKNG) - Sports betting and online gaming company\n",
      "9. Hyperloop One (HWO) - Transrapid technology company that aims to build the world's fastest mass transit system, the Hyperloop\n",
      "10. Zip Co (ZIP) - Digital payments service provider\n",
      "\n",
      "These companies are just a few examples of Musk's portfolio and holdings in various industries.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "def ask_ollama(prompt):\n",
    "    chat_history.append({'role': 'user', 'content': prompt})\n",
    "    response = ollama.chat(model='tinyllama', messages=chat_history)\n",
    "    reply = response['message']['content']\n",
    "    chat_history.append({'role': 'assistant', 'content': reply})\n",
    "    return reply\n",
    "\n",
    "# Test\n",
    "print(ask_ollama(\"Who is Elon Musk?\"))\n",
    "print(ask_ollama(\"What companies does he own?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606b4f29-2476-49cc-a83f-88e1bd273fc4",
   "metadata": {},
   "source": [
    "**ollama.generate()**\n",
    "\n",
    "-  No history, just prompt + response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e96446fd-0fcd-421d-8db7-28951976328e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photosynthesis is the process by which plants and some other organisms convert light energy (photons) into chemical energy through a series of chemical reactions, known as ATP synthesis. This process is essential for the survival and growth of organisms since it provides them with the necessary energy to carry out their daily activities. In photosynthesis, light energy from sunlight is converted into chemical energy in the form of glucose (carbohydrate), which is then used by the plant to build new structures, or starches, or energy reserves for energy storage.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.generate(\n",
    "    model='tinyllama',\n",
    "    prompt='Explain photosynthesis in one sentence.'\n",
    ")\n",
    "\n",
    "print(response['response'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aef615e-b5ee-4f10-87eb-525e533b6c84",
   "metadata": {},
   "source": [
    "**ollama.pull()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c3c517-a513-4237-82b5-bd50f0997e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "ollama.pull('llama3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac50944-f9ae-4c29-a947-74270abce881",
   "metadata": {},
   "source": [
    "**ollama.list()**\n",
    "\n",
    "- Lists all models you’ve downloaded to your local system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51bbe31d-1f4f-4861-9f81-b2eca0df8a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tinyllama:latest'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "models = ollama.list()\n",
    "models['models'][0].model\n",
    "models['models'][1].model\n",
    "# for model in models['models']:\n",
    "#     print(model['name'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818c7832-ef98-4dd4-9270-427aeed708e6",
   "metadata": {},
   "source": [
    "**ollama.show()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d14db26e-cea0-4b2e-b457-360b9f363cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modified_at=datetime.datetime(2025, 5, 28, 14, 22, 1, 722848, tzinfo=TzInfo(+05:30)) template='<|system|>\\n{{ .System }}</s>\\n<|user|>\\n{{ .Prompt }}</s>\\n<|assistant|>\\n' modelfile='# Modelfile generated by \"ollama show\"\\n# To build a new Modelfile based on this, replace FROM with:\\n# FROM tinyllama:latest\\n\\nFROM C:\\\\Users\\\\omkar\\\\.ollama\\\\models\\\\blobs\\\\sha256-2af3b81862c6be03c769683af18efdadb2c33f60ff32ab6f83e42c043d6c7816\\nTEMPLATE \"<|system|>\\n{{ .System }}</s>\\n<|user|>\\n{{ .Prompt }}</s>\\n<|assistant|>\\n\"\\nSYSTEM You are a helpful AI assistant.\\nPARAMETER stop <|system|>\\nPARAMETER stop <|user|>\\nPARAMETER stop <|assistant|>\\nPARAMETER stop </s>\\n' license=None details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='1B', quantization_level='Q4_0') modelinfo={'general.architecture': 'llama', 'general.file_type': 2, 'general.parameter_count': 1100048384, 'general.quantization_version': 2, 'llama.attention.head_count': 32, 'llama.attention.head_count_kv': 4, 'llama.attention.layer_norm_rms_epsilon': 1e-05, 'llama.block_count': 22, 'llama.context_length': 2048, 'llama.embedding_length': 2048, 'llama.feed_forward_length': 5632, 'llama.rope.dimension_count': 64, 'llama.rope.freq_base': 10000, 'tokenizer.ggml.bos_token_id': 1, 'tokenizer.ggml.eos_token_id': 2, 'tokenizer.ggml.merges': None, 'tokenizer.ggml.model': 'llama', 'tokenizer.ggml.padding_token_id': 2, 'tokenizer.ggml.scores': None, 'tokenizer.ggml.token_type': None, 'tokenizer.ggml.tokens': None, 'tokenizer.ggml.unknown_token_id': 0} parameters='stop                           \"<|system|>\"\\nstop                           \"<|user|>\"\\nstop                           \"<|assistant|>\"\\nstop                           \"</s>\"' capabilities=['completion']\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "info = ollama.show('tinyllama')\n",
    "print(info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36984974-1835-4d59-b977-60e126c90491",
   "metadata": {},
   "source": [
    "**ollama create**\n",
    "\n",
    "- create a custome model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1f217db-be61-4fcc-a94d-73955688de72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "ollama.create(\n",
    "    model='my-tutor-bot',            # Name of your custom model\n",
    "    from_='tinyllama',               # Base model (note the underscore!)\n",
    "    system='You are a friendly AI tutor who explains in simple terms.',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba0980ad-a0a0-453e-8e36-8df7a061003c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overfitting is a common problem in machine learning (ML) that occurs when the training data provides insufficient information to train the model correctly. In other words, it happens when the model's parameters or weights are not sufficient to distinguish between new data points and the training data.\n",
      "\n",
      "The main reason for overfitting is the lack of labeled data. When only a small portion of the data is labeled, it may be difficult for the machine learning algorithm to distinguish new data points from the rest. This leads to an overly complex model that does not generalize well to unseen data.\n",
      "\n",
      "In simple terms, the model tries to fit all the training data patterns in order to predict them. In reality, it's common for the model to struggle with novel patterns, and the performance degrades over time as it struggles with new data. This is called underfitting or oversimplication, which means that the model is not learning enough from the training data.\n",
      "\n",
      "There are several ways to prevent overfitting:\n",
      "\n",
      "1. Use a large number of labeled data points. Large datasets provide enough information to train and generalize effectively.\n",
      "\n",
      "2. Implement regularization techniques such as L1, L2 or elastic net regularization. These techniques try to balance the trade-off between simplicity and overfitting by adding extra constraints to the model.\n",
      "\n",
      "3. Use a neural network architecture that is well-suited for the task. It's essential to choose an architecture that is capable of handling different types of data, such as images or text data.\n",
      "\n",
      "4. Monitor the model's performance over time. Keep track of how it performs on a regular basis, and adjust your algorithm accordingly if it is not improving.\n",
      "\n",
      "In conclusion, overfitting is a common problem in ML that occurs when the training data provides insufficient information to train the model correctly. To prevent overfitting, you should use labeled data, implement regularization techniques, use a neural network architecture tailored for the task, and monitor performance regularly.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='my-tutor-bot',\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'Explain overfitting in machine learning'}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "380f85bf-ed10-4885-8333-c1026d071f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models=[Model(model='my-tutor-bot:latest', modified_at=datetime.datetime(2025, 7, 18, 22, 48, 5, 939356, tzinfo=TzInfo(+05:30)), digest='bce86acbedd90b328c986faa4e9c87cb612fc16c3152e008cfafe3b42c868cd1', size=637700166, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='1.1B', quantization_level='Q4_0')), Model(model='nomic-embed-text:latest', modified_at=datetime.datetime(2025, 5, 28, 16, 59, 31, 381740, tzinfo=TzInfo(+05:30)), digest='0a109f422b47e3a30ba2b10eca18548e944e8a23073ee3f3e947efcf3c45e59f', size=274302450, details=ModelDetails(parent_model='', format='gguf', family='nomic-bert', families=['nomic-bert'], parameter_size='137M', quantization_level='F16')), Model(model='tinyllama:latest', modified_at=datetime.datetime(2025, 5, 28, 14, 22, 1, 722848, tzinfo=TzInfo(+05:30)), digest='2644915ede352ea7bdfaff0bfac0be74c719d5d5202acb63a6fb095b52f394a4', size=637700138, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='1B', quantization_level='Q4_0'))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'my-tutor-bot:latest'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "models = ollama.list()\n",
    "print(models)\n",
    "models['models'][0].model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fdc0da-a722-4754-8f04-fa7736ad882f",
   "metadata": {},
   "source": [
    "**ollama.delete**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "30c42573-0706-45ab-9212-929f00df6d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StatusResponse(status='success')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "ollama.delete('my-tutor-bot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d850fad-7c7e-45d9-a168-5d1f53a16ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
